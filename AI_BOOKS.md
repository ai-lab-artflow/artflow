---


---

<h1> 아트플로우 
</h1> <h3>1. 랩 이름이 뭔가요? </h3>
<strong>Artflow</strong>
<p><img src="https://lh3.googleusercontent.com/MGChlOz8zgG7wpazeaEH13FufFDv55TUnd5dUAVr7p_fHiOxys4br92Fbri0rcl7ZQ2OARlekZg6=s500" alt="" title="artflow"></p>
 <p></p><h3>2. 랩 이름을 왜 이렇게 지었나요?</h3>
 art라는 것은 우리나라에서는 예술이란 의미로 많이 쓰이지만 프랑스는 기술, 독일은 종류라는 의미로 많이 쓰여요. 그래서 우리는 art라는 예술과 AI의 흐름, AI를 위한 다양한 소프트웨어(종류)라는 의미와 flow라는 ‘흐르다’라는 의미가 합쳐져 ‘Artflow’가 탄생되었어요.<p></p>
<h3>3. 랩원들을 소개해주세요.</h3>
<p><img src="https://lh3.googleusercontent.com/vndxluTkxbMebUV08DUGXcnut2VlSt_6IEyWb2L7K3KqRdhUWZBBbzOcBXz7bX9-MmAFIv2nGN4K=s50" alt="" title="김현우"> <strong>김현우</strong> :</p>
<p>
2016년 3번의 도전 끝에 한양대학교 정보시스템학과에 입학한 3월, 세상은 이세돌과 알파고의 바둑 대결로 들썩이고 있었습니다. 많은 전문가가 이세돌의 승리를 점쳤지만, 구글 딥러닝 전문가들이 개발한 바둑 AI 알파고는 손쉽게 이세돌을 이겼고, 사람들은 4차 산업혁명 시대가 도래했음을 몸소 느낄 수 있었습니다. AI는 이제 영화에서 보던 뜬구름 잡는 이야기가 아닌 실생활에 깊숙하게 다가왔고, 네이버를 포함한 많은 기업이 AI 산업에 박차를 가하고 있습니다.
</p><p>제가 가장 관심 있는 분야는 인공지능 기반의 사물인터넷입니다. 모든 사물의 연결은 스마트 가전과 홈을 넘어 자동차와 도시 인프라까지 퍼지고 있습니다. 구체적으로 모바일 디바이스, 집, 다른 차량, 도로 인프라 등 자동차 주변의 사물들과 연결하는 커넥티드 카(Connected car)를 궁극적으로 실현하고 싶습니다.</p>
<p>제가 다니는 정보시스템학과의 커리큘럼은 매우 훌륭하지만, 인공지능 기반의 전문가로 성장하기에는 부족한 면이 많습니다. 자동차 임베디드 개발자가 되기 위해 머신러닝, 딥러닝에 대한 전반적인 지식을 쌓고 뜻이 많는 개발자들과 같이 협업하고자 ‘한국인공지능연구소’의 ‘아트플로우 랩실’에 참가하게 되었습니다.</p>
<p><img src="https://lh3.googleusercontent.com/ygIgFegvocrtsJliiT7DX_XdbKIv6RnYeX72ADQGJska2iWluf97v_jTKS6YeXWPojkSASKj5yf2=s50" alt="" title="김훈민"><strong>김훈민</strong> :</p>
<p>안녕하세요 ’한국인공지능연구소’ 4기 연구원으로 활동한 김훈민입니다. 원래 제 직업은 PLC 소프트웨어 엔지니어였어요. 그런데 직장 생활 중 비전 시스템에 딥러닝을 적용한 것을 보고 관심이 생겨 공부를 시작하게 됐습니다. 소프트웨어로 뉴런 구조를 프로그램해서 학습이 가능하다는 게 처음엔 아주 신기했거든요. 연구원 활동을 시작할 때만 해도 직장 생활을 하고 있었는데 공부를 하다 보니 재미를 느껴 인공지능 분야로 이직을 생각하게 되어 일을 그만두고 공부를 하며 이직 준비를 하고 있어요. 처음 ‘한국인공지능연구소‘를 알게 된 건 우연히 중국 출장 중 같이 일하던 협력업체 분께서 딥러닝에 관심이 있어 인공지능 분야에 대해 이야기를 나누다가 한국 복귀할 때 추천을 해줘서 참여하게 되었어요. 연구소 활동을 시작할 때는 실력이 부족하다 느껴 참여에 부담감을 가지고 있었는데, 처음 관심을 가졌던 분야가 이미지 처리이고 마침 아트플로우 랩장님께서 환영하며 받아주셔서 즐겁게 활동을 하고 있습니다!</p>
<p><img src="https://lh3.googleusercontent.com/gl7bkkPmOIthPt1z_fA9jCx4whqnUwIaI23ItYk1ygz3JAxn7av5soLWa_JU9oLoTKcv4OIiIuMC=s50" alt="" title="민예린"> <strong>민예린</strong> :</p>
<p>
5기에 아트플로우 랩에 들어오게 된 민예린입니다. 전공은 수학·금융이고, 주 관심 분야는 강화학습과 자연어입니다 :)
</p><p>한국인공지능 연구소에 들어오기 전에는 RNN 공부를 잠깐 했었고, 4기 땐 ‘주가 예측’을 하는 알파 프로젝트 랩에서 RNN으로 주가 예측을 공부를 하다 기수가 끝나갈 즈음에 강화학습을 접하게 되어 흥미를 가지게 되었습니다. 그러다 이전 랩이 사라지고, 데모데이 때 아트플로우에서 강화학습을 공부한다는 발표를 듣고 참여를 결심하게 됐습니다. 무엇보다 랩원분들이 무척 좋아 보여서 재미있는 활동이 될 것 같습니다 :)</p>

<p><img src="https://lh3.googleusercontent.com/JCElC-S7Oh8ERUkNxE99JqpL7bP7_naNibkPHEaSnIMhUCbIwrlM_YToiWoDfb5YKPOykktOJ1-r=s50" alt="" title="박성은"><strong>박성은</strong> :</p>
<p>사실 지도교수님을 쭐레쭐레 쫒아와서 &lt;한국인공지능연구소&gt;에 대해 알게되었어요.
</p><p>학부 전공은 악기를, 석박사과정에서는 이과계열(경제학, 통계학)을 공부하는 중이지만 컴퓨터 공학 쪽은 정말 접할 거라곤 생각하지도 못했었죠.</p>
<p>그런데 생각보다 코드들이 악보와 비슷한 점이 많아서 이에 빗대어 공부했을 때 재밌는 매력을 찾게 되었습니다. 또 공부하는 통계학과도 비슷한 느낌이라서 현재까지 즐겁게 임하고 있어요. </p>
<p><img src="https://lh3.googleusercontent.com/EEh6AtQW8AOdzrf_JzB1bXZ9gyHkUlXc__MyiTId_281HuMyQvmqZKGfW6_T1hPsBgYQ0rtv3e0k=s50" alt="" title="윤현근"><strong>윤현근</strong> :</p>
<p>저는 스마트 팩토리 업체에서 개발자로 일하고 있는 윤현근이라고 합니다. 개발자라고는 하지만 일을 한 기간이 길지 않아 아직 많이 부족한 새내기 개발자입니다. 저는 오랫동안 고시 공부를 하였습니다. 하지만 원하던 결과를 얻지 못하였고 공부를 그만두고는 한참 방황을 하다 데이터 사이언스 분야로 들어오게 되었습니다. 대학교에서 인공지능 수업을 들었던 적이 있는데 그 당시에는 교수님께서 이론을 설명하시고는 늘 끝에는 이론만 있을 뿐 여러 가지 문제로 인해 구현이 잘되지 않는다는 말을 함께 하셨습니다. 하지만 최근 몇 년 사이에 이게 가능할까라고 생각했던 것들이 실제로 만들어지는 것을 보면서 대학교를 다닐 때 가졌던 호기심을 다시 가지게 되었고 남들보다 많이 늦었지만 하고 싶은 일을 해보자는 생각으로 공부를 시작하게 되었습니다.
</p><p>처음에는 혼자서 인터넷을 통해 공부를 하였지만 생각보다 쉽지 않았고 우연히 패스트 캠퍼스 광고를 보게 되었고 충동적으로 데이터 사이언스 스쿨 과정을 수강하게 되었습니다. 짧은 시간에 많은 내용을 소화해야 하는 쉽지 않은 과정이었지만 4개월 동안 다양한 분야의 사람들이 한 곳에 모여 함께 공부하면서 프로젝트를 진행하고 많은 생각과 고민을 공유하기도 하고 때로는 서로를 위로하기도 하는 시간들이 잊지 못할 추억이 되었습니다. 교육 과정이 끝나고 배워야 할 것이 아직도 너무나 많았고 무엇보다도 배움에 대한 순수한 마음을 가진 사람들과 함께 할 수 있는 방법이 없을까 찾아보다 ’한국인공지능연구소‘를 알게 되었고 낯가림이 심해 많이 주저했지만 용기내 참여하게 되었고 ’아트플로우’에서 활동하게 되었습니다.</p>
<p><img src="https://lh3.googleusercontent.com/KuG8ITu2uYEPzkFN8heXISyqcljp073gjwTBxX7L_KsLPUv_LkGyP2mwmVubFu_NlfQT21Nh2j1P=s50" alt="" title="이준석"><strong>이준석</strong> :</p>
<p> 평소 개발에 관심이 많아서, 이곳 저곳 개발할 곳을 찾고 있었습니다. 그러던 중, 저희 과 선배인 전민종님께서 아트플로우 랩실을 추천해줬고, 마침 인공지능에 흥미가 생기던 중이라 팀에 합류하게 되었습니다. 처음에는 인공지능의 개념들이 생소해 공부하는데 어려움을 겪었지만, 몇 번 발표를 준비하고 공부를 하면서 흥미가 생기게 되었습니다. 아직 정해진 진로는 없지만, AI에 대해 좀 더 오래 공부해보고, 적성이 맞다면 인공지능 관련 일에 종사하고 싶습니다. </p>
<p><img src="https://lh3.googleusercontent.com/7lsIIljE5rgp8KzskVHL7LORoL2Bb4u-PjVSwGzgf1I7h-4fHWknz64e5pNsQJIB6nr8DqZ3bqGS=s50" alt="" title="이지은"><strong>이지은</strong> :</p>
<p>산업공학을 전공하고 있는 대학생입니다. 다양한 분야의 수업을 듣다가 IT 분야에 흥미를 느꼈고, 이론에서 끝나는 수업에서 더 나아가 실제로 적용해보는 프로젝트에 참여해보고 싶다고 생각하던 때에 과 선배를 통해 ‘한국인공지능연구소’에 대해 알게 되었습니다. 초보자에게도 활짝 문이 열려 있고, 함께 이런저런 시도를 해보자는 ‘아트플로우’의 취지에 반해 합류하게 되었어요. 이 분야가 알면 알수록 배울 것이 무궁무진하고 신기한 것들이 많아 즐겁게 공부하고 있습니다.</p>
<p><img src="https://lh3.googleusercontent.com/jkArkChfIMdrRwGHpQz-msVMRffZGFUdzlFCb2f4J2dCRPbP6_gQvPF5Cqq-gO2rDLH4FLPZ3fCc=s50" alt="" title="임하경"><strong>임하경</strong> :</p>
<p>저는 원래 인공지능에는 별 관심이 없었고 코딩이라는 것도 어렵고 지루한 것이라고만 생각하고 있었습니다. 그런데 알파고 이후 인공지능은 정말 빨리 발전하는 것 같았고 이러다가 정말 나중엔 의사도 인공지능에 의해 완전히 대체되지 않을까 하고 위기의식도 많이 느꼈습니다. 병원에서 일하시는 교수님들도 그렇게 많이 느끼신 것 같았습니다. 그래서 인공지능, 딥러닝이라는게 도대체 뭐길래 이런 변화를 가져올 수 있는 걸까, 항상 궁금했습니다. 그러던 와중에 제 친구 민종이가 한국인공지능연구소에 같이 참여 하자고 하였고 저도 좋은 기회라는 생각에 연구활동에 참여하게 되었습니다.</p>
<p><img src="https://lh3.googleusercontent.com/RICyunnCjhRK8WYrUhJQkeFJ2uk0aNgr_wR7LiD_vUnFmSALHqJPHVfKdtNhgF6_xHeBd9heeZf6=s50" alt="" title="임한동"><strong>임한동</strong> :</p>
<p>기계공학도로서 하드웨어만 주로 다루다가 학교에서 진행한 파이썬 
강의를 우연히 듣게 되었는데 그때부터 소프트웨어 분야, 특히 
인공지능 관련 분야를 공부하기 시작했습니다. 그리고 향후 로보틱스 분야 대학원으로 진학 예정이기에 관련 공부를 더 해보고 싶어 
‘한국인공지능연구소’에 참여하게 되었고 그중에서도 비전을 중심으로 연구하는 ‘아트플로랩‘에 들어오게 되었습니다.</p>
<p><img src="https://lh3.googleusercontent.com/6E48N4HzEvzux8H5qd9jFC96tkEKqXH7DBg8gnAtfoe5E-0FU2PZgjO5kmEJyY9xZOabPUrs28ME=s50" alt="" title="전민종"><strong>전민종</strong> :</p>
<p>평소에 인공지능에 관한 뉴스나 소식들을 들으면 ‘어차피 내 일 아니니까~’하며 항상 별로 신경 쓰지 않았습니다. 이후 대학원 관련으로 교수님과 상담을 한 후, 학부 연구생으로 지내는 와중에 인공지능 관련 과제를 내주셔서 이 공부를 시작하게 되었습니다. 그리고 우연히
페이스북을 통해 &lt;한국인공지능연구소&gt;를 알게 되었고, 현재 ‘아트플로우’라는 좋은 랩과 랩원들을 만나게 되어 즐겁게 공부에 임하고 있습니다. 앞으로 대학원 진학 후에 강화학습 관련으로 금융 쪽 연구를 하고싶으며, 이어 박사까지 나아갈 예정입니다.</p>
<p><img src="https://lh3.googleusercontent.com/mOf9s-aE5jf0XkAaapzsioKBE4eMQSeTGNXjDEagMzn5WmcNrfCYlNZB3xlqm3sjOLH6Qu1KBh9V=s50" alt="" title="조원양"><strong>조원양</strong> :</p>
<p>저는 20년 동안 SW개발을 했습니다. 새로운 것을 시도하는 것을 좋아해서 다양한 분야와 플랫폼에서 새롭게 개발을 하는 것을 좋아했습니다. 그러던 중, 몇 년 전 이세돌 기사와 알파고의 바둑 대결을 보고 슬슬 공부를 해야겠다는 생각이 들었습니다. 하지만 그때 당시 깊고 집중해서 공부하지는 않았습니다.
</p><p>일을 하면서 주로 사용했던 프로그래밍 랭귀지가 C++이었고 또 PC나 모바일 등 다양한 플랫폼에서 어플리케이션 개발이 가능했기 때문에 미래에 대해 그다지 걱정을 하진 않았습니다.</p>
<p>그러던 중에 2018년도 초에 2-3년 뒤 제 모습을 생각해보니 이제는 기계 학습, 딥러닝에 대해 전문적인 지식을 쌓지 않으면 그저 그런 엔지니어가 될 것 같다는 생각이 들었습니다. 그래서 다시 시중에 출판되어 있는 책을 보기 시작했는데, 처음에는 방향을 잡지 못했습니다. SNS에서 여러 스터디 모임을 모집하는 것을 봤지만 자신감이 없어 참여를 하지 못했습니다.</p>
<p>계속 여러 시도를 하다가 cs231n 수업, “신경망 첫걸음 – 수포자도 이해하는 신경망 동작 원리와 딥러닝 기초 (한빛미디어, 2017)” 그리고 “밑바닥부터 시작하는 딥러닝 (한빛미디어 2017)” 책을 보면서 개념을 잡기 시작했습니다. 또한, 코세라 딥러닝 강의, 유다시티의 나노디그리 딥러닝 과정을 들으면서 이제는 스터디에 참여해서 같이 공부를 해야겠다고 생각을 했습니다. 그 때 때마침 한국인공지능연구소에서 3기 연구원 모집을 시작해서 배우면서 같이 공부를 해야 겠다는 생각이 들어 참여를 했습니다.</p>
<p>처음에 참여를 했을 때는 제가 무엇을 하고 싶은지, 또 무엇을 해야 할지 몰랐습니다. 제가 일하는 분야를 고려하면 객체 인식, 얼굴 인식/식별 그리고 트래킹 등 비전에 대한 분야를 하는 것이 맞지만, 기계학습과 딥러닝의 다양한 분야를 겪어 보고 좀 다른 분야를 연구해보고 싶었습니다. 그래서 3기 참여시 선뜻 랩을 결정하지 못하고 남아 있다가 “아트플로우” 랩에 참여를 하게 되었습니다.</p>
<p>3기, 4기를 거치면서 이제는 영상 처리, 자연어처리 그리고 강화학습등 다양한 딥러닝 기술을 융합해야겠다고 생각하고 있습니다.</p>
<h3>4. 랩 주제는 뭔가요?
</h3><p>아트플로우의 연구 주제는 <strong>‘비전 + 자연어처리 + 강화학습’</strong>입니다.</p>
<h3>5. 랩 주제를 설명해주세요.</h3>
<p>저희 아트플로우는 어느 한 분야에 국한된 것이 아니라 다양한 기계학습 이론을 융합해서 새로운 연구결과를 산출하는 것을 목표로 합니다. 한 예로 최근에 보도된 시각 장애우가 카메라를 착용하고 있으면 보고 있는 방향을 분석해서 문장으로 만들고 그것을 다시 음성으로 알려주는 응용 프로그램을 만드는 것이 저희 랩의 주제와 비슷합니다.</p>
<h3>6. 랩 결성 초반의 연구활동 계획은 무엇이었나요?</h3>
<p>처음에 아트플로우는 단순히 딥러닝을 알고 싶고 공부하고 싶어 하는 연구원들로 구성이 되었습니다. 그리고 대부분이 그전에는 공부를 해본 적이 없는 연구원들이었습니다. 그래서 처음에는 그나마 조금 알고 있다는 저와 한분이 텐서플로우와 파이썬을 나눠서 가르쳐 주는 형태로 진행을 했습니다. 그리고 그나마 제가 조금 알고 있는 분야 였던 객체 인식에 대해 오픈 소스를 구해서 한번 학습시키고 구동시켜 보자는 것에 초점을 맞춰서 진행을 했습니다.</p>
<h3>7. 중반 이후 연구 방향이나 활동에 변화가 있었나요?</h3>
<p>3기에는 단순히 스터디와 한 분야에 대한 실습을 진행했다면 그 이후에는 특정 한 분야에 국한 하는 것이 아니라 다양한 시도를 하는 것으로 연구방향을 설정했습니다. 그래서 진행되는 기수 마지막 모임 때 다음 기수에서 진행할 연구 주제나 방법을 토의를 합니다.
</p><p>기본 활동은 “이론 및 최신 연구 방향 연구를 위한 논문 리뷰 + 기본기를 다지기 위한 스터디 + 구현” 이 세 부분으로 진행을 합니다. 각 세부 활동에는 활동을 활성화하는 퍼실리티를 배정하고 진행을 합니다.</p>
<h3>8. 랩 모임을 하면서 에피소드가 있었다면?</h3>
<p><img src="https://lh3.googleusercontent.com/1kd62PD4v3zemne3ezoOCYS47e8xULOOK_fyHTxQChCxb7hd2RcWbhDr_Bh2GxOKfVe-4ai4bT6Z=s50" alt="" title="김훈민"><strong>김훈민 </strong> :</p>
<p>랩 모임을 하면서 가장 기억에 남는 에피소드라면 4기 마지막 랩 모임을 마칠 때 회식을 했었거든요 좋은 분들과 좋은 연구주제로 모인 팀인 만큼 마음이 잘 맞고 이야기도 잘 통해 회식을 하다 보니 평소보다 과음을 하게 되었어요… 다음날 조금 힘들었지만 재미있었던 기억으로 남은 회식이었답니다. 다음 회식도 기대되네요.</p>
<p><img src="https://lh3.googleusercontent.com/gl7bkkPmOIthPt1z_fA9jCx4whqnUwIaI23ItYk1ygz3JAxn7av5soLWa_JU9oLoTKcv4OIiIuMC=s50" alt="" title="민예린"> <strong>민예린</strong> :</p>
<p>
4기 때 주가 예측을 하던 랩이다 보니 랩장님이 catboost로 주요 기업 주가의 상승 하락 예측을 매일 하셨는데, 가끔 모임을 할 때면 데일리 예측과 실제 변동을 보여주시면서 아쉬워하시는 모습이 아무래도 가장 기억에 남는 에피소드입니다 헤헤.
</p>
<p><img src="https://lh3.googleusercontent.com/JCElC-S7Oh8ERUkNxE99JqpL7bP7_naNibkPHEaSnIMhUCbIwrlM_YToiWoDfb5YKPOykktOJ1-r=s50" alt="" title="박성은"><strong>박성은</strong> :</p>
<p>2기수(약 6개월) 동안에 2주에 한번씩 만나면서 정.말.(궁서체) 스터디만 했습니다.
</p><p>얼굴만 알고 인사만 하는 정도였는데 마지막 기수날에 다함께 저녁을 먹으며 술 한잔을 걸치자며 누군가가(사실은 제가) 제안했죠.</p>
<p>6개월 만에 서로 많은 대화를 나누며 맛있는 저녁 식사와 알코올을 섭취하게 되었어요. 정말 재밌는 시간이었는데 마지막을 기억 못 하시는 분들도 몇 분 계실 듯 하네요. 하하.</p>
<p><img src="https://lh3.googleusercontent.com/KuG8ITu2uYEPzkFN8heXISyqcljp073gjwTBxX7L_KsLPUv_LkGyP2mwmVubFu_NlfQT21Nh2j1P=s50" alt="" title="이준석"><strong>이준석</strong> :</p>
<p>케라스 스터디를 할 때 8장 발표를 할 사람이 정해지지 않았던 경우가 있었습니다. 이미 저는 케라스 발표를 한 번 하기도 했고, 시험 끝난지 얼마 안됐어서 준비할 시간이 애매해 나서지 않고 조용히 있었습니다. 그 때! 저희과 선배님이신 전민종님이 저를 적극 추천해주셔서 계획에도 없던 발표를 한번 더 하게 되었습니다. 그리고 이번에도 전민종님의 추천 덕분에 시험 4일 전날 발표를 합니다^^ 감사합니다. 전민종님!</p>
<p><img src="https://lh3.googleusercontent.com/RICyunnCjhRK8WYrUhJQkeFJ2uk0aNgr_wR7LiD_vUnFmSALHqJPHVfKdtNhgF6_xHeBd9heeZf6=s50" alt="" title="임한동"><strong>임한동</strong> :</p>
<p>랩 활동 초반에 아는 것도 없는데 랩장님이 저에게 논문 요약 발표 기회를 주셨습니다. 발표를 잘 할 수 있을까 걱정을 많이 했는데 준비하는 과정에서 많이 배웠던 것 같고 제가 발표한 부분이 3개월 활동 동안 가장 기억에 많이 남았습니다. 인생은 역시 일단 부딪히고 봐야 된다는 인생 교훈을 또 얻었습니다.
</p><p>그리고 4기 마지막 날 회식을 했었는데 다음 기수 랩장을 민주적인 사다리타기 게임으로 정한 것이 가장 인상적이었습니다. 왜냐하면 제가 당첨되었기 때문이죠. 전혀 생각지 못하고 있었는데 이런 뜻밖의 행운의 기회가…호박이 넝쿨째 굴러들어와서 잊지 못할 밤이었던 것 같습니다.</p>
<p><img src="https://lh3.googleusercontent.com/jkArkChfIMdrRwGHpQz-msVMRffZGFUdzlFCb2f4J2dCRPbP6_gQvPF5Cqq-gO2rDLH4FLPZ3fCc=s50" alt="" title="임하경"><strong>임하경</strong> :</p>
<p>가장 기억에 남았던 건 랩원들끼리 모여서 처음으로 코드를 돌려보았을 때였습니다. Retinanet이라는 코드였는데 collaboratory로 돌리자 마자 오류가 떴습니다. 그래서 쥬피터 노트북으로 다시 돌려보았는데 또 실행되지 않았고 그 뒤로는 tensorflow를 다른 버전으로 깔아보고, cuda도 깔아보고 하면서 코드 하나 실행해보려고 별의 별 짓을 다 했습니다. 결국 저희는 두시간 동안 노트북과 씨름하다가 결국 그 코드는 돌리지 못했습니다. 저는 그때 처음으로 코드 하나 실행하는 것도 쉬운게 아니라는 것을 느꼈고 개발활경 구축하는 것부터 제대로 배워야겠다고 생각했죠, 그때 경험이 있었기 때문에 나중에 코드를 돌릴 때 오류가 나더라도 당황하지 않고 의연하게 대처할 수 있었던 것 같아요,</p>
<p><img src="https://lh3.googleusercontent.com/6E48N4HzEvzux8H5qd9jFC96tkEKqXH7DBg8gnAtfoe5E-0FU2PZgjO5kmEJyY9xZOabPUrs28ME=s50" alt="" title="전민종"><strong>전민종</strong> :</p>
<p>저에게 가장 인상깊은 에피소드라 하면… 아트플로우는 항상 모임이 합정이나 공덕에서 진행되었습니다. 이게 왕복이 3시간이라 저한테 굉장히 멀다고 느껴졌습니다. 스터디의 랩 활동은 4시간 남짓 안되는데 한번은 공덕에 지하철을 타고 가다가 ‘진짜 도저히 안되겠다!’라는 마음이 들어, 그날 바로 장소를 바꾸자는 건의를 하였습니다. 칠판에 각자 사는 곳을 적어 놓고 중간 지점을 정하는데 마침 다행히도 각자 사는 곳이 비슷해서 드디어 공덕, 합정을 탈출했습니다! 이런 건의를 한 제가 너무 뿌듯하고 자랑스럽고 대견스럽습니다. 우리 앞으로는 사당에서 만나요!</p>
<p><img src="https://lh3.googleusercontent.com/mOf9s-aE5jf0XkAaapzsioKBE4eMQSeTGNXjDEagMzn5WmcNrfCYlNZB3xlqm3sjOLH6Qu1KBh9V=s50" alt="" title="조원양"><strong>조원양</strong> :</p>
<p>제일 생각이 많이 나는 것은 처음에 결성될 때입니다. 다들 뭘 해야 할지 몰라서 남아있던 연구원들이었고 대부분 파이썬을 설치조차 해본 적이 없던 연구원들이 많았습니다. 다들 모였는데 현업 개발자라는 이유만으로 임시 랩장을 맡았었네요. 지금은 규칙을 바꿔서 돌아가면서 랩장을 하기로 했습니다...</p>
<h3>9, 10, 11, 12. 다음의 랩 결과물에 대해 설명해주세요.(코드와 설명)</h3>
<p>
저희 아트플로우는 매주 책의 챕터들과 논문을 발표 하였는데요, 하나는&lt;케라스 창시자에게 배우는 딥러닝 – 프랑소와 숄레&gt;이고 다른 하나는 객체 탐지에 관련된 논문 리뷰입니다. 우선 책의 챕터들에 대한 리뷰를 먼저 살펴보겠습니다.</p>
<p><img src="https://lh3.googleusercontent.com/1kd62PD4v3zemne3ezoOCYS47e8xULOOK_fyHTxQChCxb7hd2RcWbhDr_Bh2GxOKfVe-4ai4bT6Z=s50" alt="" title="김훈민"><strong>김훈민</strong> :</p>
<p>RNN(Recurrent Neural Networks) 순환 신경망이란 결과값을 출력층 방향으로도 보내면서 다시 은닉층의 다음 Time step의 입력으로 보내는 특징을 갖습니다. 이러한 특징 때문에 이전의 연산이 이후 연산에 영향을 미치는 구조이고 자연어나 시계열 데이터와 같이 앞선 데이터가 현재 처리 결과에 영향을 주는 데이터처리에 효율적입니다.
</p><p><img src="https://lh3.googleusercontent.com/fJbjCDGoiN0ovPkuPcHt-p81RH7GbSsWeTOy6wrfWi_5E0pFTtmfrN9_CDpwk437JfS3zfZHx-2j=s700" alt="" title="simple rnn"></p>
<center> Simple RNN(Recurrent Nueral Networks)구조</center>
<p>RNN의 학습에 사용되는 알고리즘인 Backpropagation through time의 경우 각 Time step마다 오류를 측정하고 이전 step으로 전파합니다. 처리하고자 하는 데이터가 길어질수록 역전파시 기울기가 0에 아주 가까운 값으로 수렴하게 되는 기울기 소실 문제(Vanishing gradient problem)가 발생할 것입니다. 그리고 Time Step이 길어질수록 예전에 있던 정보를 기억하지 못하게 됩니다.</p>
<p>이러한 RNN의 문제점을 보완하기 위해 나온 LSTM(Long Short-Term Memory)은 여러 개의 게이트가 합쳐져 있는 구조며 선택적으로 정보들이 기억될 수 있도록 합니다. 먼저 이전 step에서 기억하는 cell state의 정보를 얼마나 잊을지 결정하고 새로운 정보를 얼마나 기억할지 결정하여 cell state에 추가합니다. 그리고 앞의 결과에 의해 정해진 cell state와 입력을 이용하여 어떤 값을 출력할지 결정하게 되는 구조입니다. 이렇게 결정되는 cell state는 장기적으로 이전 상태들을 기억 할 수 있게 도와주는 역할을 합니다.<br>
<img src="https://lh3.googleusercontent.com/Mvaim9wL82Lb1Xqa7jwb13hTMqT2ONQr3aiMLdDMuq-qSdv3qdyLVNjEapEqGcCharsIs4lfiO9E=s700" alt="" title="lstm"></p>
<center>LSTM(Long Short-Term Memory) 구조</center> <br>  
<p><img src="https://lh3.googleusercontent.com/EEh6AtQW8AOdzrf_JzB1bXZ9gyHkUlXc__MyiTId_281HuMyQvmqZKGfW6_T1hPsBgYQ0rtv3e0k=s50" alt="" title="윤현근"><strong>윤현근</strong> :</p>
<p>
</p><center>Convolutional Neural Network</center>
<ol>
<li>CNN</li>
</ol>
<p>CNN은 1980년대부터 이미지 인식 분야에 사용되었지만 최근 들어서 컴퓨터 성능의 향상과 많은 양의 훈련데이터, 다양한 기술 덕분에 복잡한 이미지 처리 문제에 좋은 성능을 낼 수 있었습니다. 또한 시각 분야에 국한되지 않고 음성 인식, 자연어 처리 등에도 사용됩니다.</p>
<ol start="2">
<li>CNN의 구조<br>
<img src="https://lh3.googleusercontent.com/x9jKEpkBPX-MD-0ANa2sbsQlUMoEzNwckf5423dKlDObA6qhRkC2pNyn3hr3K_hTfyzLX4J1s4xf=s700" alt="" title="cnn"></li>
</ol>
<p>2.1 합성곱층(Convolution layer)</p>
<p>CNN에서 가장 중요한 구성 요소이며 입력된 이미지 전체가 아닌 부분의 값을 읽어</p>
<p>특성을 파악합니다. 여러 개의 층을 통해서 초기에는 저 수준 특성(선, 점등)을</p>
<p>파악하고 더 깊은 층을 통해서 고 수준 특성(눈, 코, 입등)을 파악하는 계층적 구조를</p>
<p>가지고 있으며 이러한 특성이 CNN을 이미지 인식에 많이 사용하는 이유입니다.</p>
<p>2.2 필터(Filter) 또는 커널(Kernel)</p>
<p>필터는 수평선, 수직선과 같이 서로 다른 특성을 가지고 있으며 입력 이미지를 필터를</p>
<p>이용하여 합성곱을 구하는 과정을 통해 필터와 유사한 이미지의 영역을 강조하는</p>
<p>특성맵(feature map)을 만들어냅니다.</p>
<p>2.3풀링(Pooling)</p>
<p>계산량과 메모리 사용량, 파라미터 수를 줄이기 위해 이미지의 축소본을 만드는</p>
<p>것입니다. 다양한 방법이 있지만 가장 널리 사용되는 방법은 Max Pooling이며 작은</p>
<p>커널에서 최대값만을 남기고 나머지 값은 버리는 방법으로 해당 영역의 가장 큰</p>
<p>특성을 파악합니다.</p>
<ol start="3">
<li>CNN 특징</li>
</ol>
<ul>
<li>
<p>파라미터 공유 : 모델의 전체 파라미터 수를 급격하게 줄여줌</p>
</li>
<li>
<p>학습된 패턴의 평행 이동 불변성 : 한 지점의 패턴을 인식하도록 학습되었다면 다른</p>
</li>
</ul>
<p>어느 위치에 있더라도 패턴을 인식, DNN은 그 위치에 있을 때만 감지</p>
<ul>
<li>계층적 구조 : 매우 복잡하고 추상적인 시각적 개념을 효과적으로 학습</li>
</ul>
<ol start="4">
<li>다양한 CNN 종류</li>
</ol>
<p>이 기본 구조를 활용하여 다양한 형태의 CNN이 개발되었고 이는 인공지능 분야의 발전을 이끌었습니다. 그 종류로는 LeNet-5, AlexNet, GoogLeNet, ResNet등이 있습니다.</p><br>
<img src="https://lh3.googleusercontent.com/KuG8ITu2uYEPzkFN8heXISyqcljp073gjwTBxX7L_KsLPUv_LkGyP2mwmVubFu_NlfQT21Nh2j1P=s50" alt="" title="이준석"><strong>이준석</strong> :
<p>RNN(Recurrent Neuron Network)과 LSTM(Long Short Term Memory)
</p><p>RNN은 기존의 신경망과 다른 독특한 특징을 가지고 있습니다. 기존의 신경망은 input이 들어가고 여러 layer들을 거쳐 output이 나오는 형태였다면, RNN은 이전 output값이 input값과 함께 고려되어 들어갑니다. 이러한 이유로 보통 RNN이 ‘기억 능력’을 갖고 있다고 이야기합니다. 하지만 RNN의 문제는 gradient를 구하는데 어려움이 있다는 점입니다. Gradient 값을 구하기 위해 미분을 하다 보면 gradient가 0이 되거나 무한대로 발산을 하는 경우가 생기게 됩니다. 이러한 문제점을 해결하기 위해 고안된 방법이 LSTM입니다. LSTM의 가장 큰 특징은 필요에 따라 기억들을 지우고 더하는 알고리즘이 있다는 점입니다. 잊어야 할 정보들은 기억에서 잊고, 추가해야 할 정보들은 추가하면서 RNN 보다 더 효율적으로 활용이 가능합니다. </p><center><img src="https://lh3.googleusercontent.com/oTHCKlEGZ3WJCV30EJtv4iXgS6iZAQQ2hKFyxRaYrps4LN2kuZhB_DmbqnoPjUD2GtdRmBPUcDGR=s700" alt="" title="rnn"><br>
GAN (Generative Adversarial System)</center><p></p>
<p>GAN은 최근 많은 주목을 받고 있는 신경망으로, 적대적 시스템으로 이미지를 생성하는 인공 신경망입니다. 적대적 시스템이라 하면, 정과 반의 모순적 대립을 통해 최적값을 찾아내는 것을 뜻합니다. GAN에는 generator와 discriminator가 있습니다. Generator는 임의의 설정된 정보(latent space)를 바탕으로 가상의 이미지를 만들어 내는 신경망 구조의 생성 시스템을 뜻합니다. Discriminator는 입력된 이미지가 진짜 이미지일 확률(0과 1사이 값)을 출력값으로 하여 일치의 정도를 출력하는 시스템입니다. 쉽게 예를 들어, 지폐 위조범은 generator, 경찰을 discriminator라고 가정했을 때, 지폐위조범은 위조 지폐를 만들어 경찰을 속이기 위해 보여줍니다. 그러면 경찰은 그 위조 지폐를 보고 이것이 진짜 지폐인지, 가짜 지폐인지를 판별합니다. 즉, 이러한 역할을 통해 진짜 같은 가짜 이미지를 만들어내는 신경망을 GAN이라고 합니다. 이는 GAN의 generator의 결과물을 우리가 원하는 데로 마음껏 조작할 수 있다는 가능성을 확인한 것이며, 단순한 데이터의 분류로서의 이해가 아닌 새로운 것을 창조할 능력을 지게 된 것을 의미합니다.<br>
<img src="https://lh3.googleusercontent.com/nYnLHMZVQ5O-oiRrAdrfX9K3hOx7TKa32H0hnHT_MTBTv_BpGo7hNJum75CGzeNpa3wjRdGgIQtK=s600" alt="" title="gan"></p>

<p><img src="https://lh3.googleusercontent.com/7lsIIljE5rgp8KzskVHL7LORoL2Bb4u-PjVSwGzgf1I7h-4fHWknz64e5pNsQJIB6nr8DqZ3bqGS=s50" alt="" title="이지은"><strong>이지은</strong> :</p>
<p>&lt;케라스 창시자에게 배우는 딥러닝 – 프랑소와 숄레 / 4장 발표 요약&gt;
</p><p>전체 머신 러닝은 복잡한 하위 분류를 가진 방대한 분야인데, 일반적으로 지도, 비지도, 자기 지도, 강화 학습의 범주로 나눌 수 있습니다.</p>
<ul>
<li>지도 학습 (supervised learning) : 가장 흔한 경우로 샘플 데이터가 주어지면</li>
</ul>
<p>알고 있는 타깃에 입력 데이터를 매핑하는 방법을 학습합니다.</p>
<ul>
<li>비지도 학습 (unsupervised learning) : 어떤 타깃도 사용하지 않고 입력 데이터에</li>
</ul>
<p>대한 흥미로운 변환을 찾습니다 데이터 시각화, 압축, 노이즈 제거 또는 데이터에</p>
<p>있는 상관관계를 더 잘 이해하기 위해 사용합니다.</p>
<ul>
<li>자기 지도 학습 (self-supervised learning) : 학습 과정에 사람이 개입하지 않는</li>
</ul>
<p>지도 학습이라고 생각할 수 있습니다. 보통 경험적인 알고리즘을 사용해서</p>
<p>입력 데이터로부터 생성합니다.</p>
<ul>
<li>강화 학습 (reinforcement learning) : 에이전트(agent)는 환경에 대한 정보를 받아</li>
</ul>
<p>보상을 최대화하는 행동을 선택하도록 학습됩니다. 현재 강화 학습은 대부분 연구</p>
<p>영역에 속해 있고 게임 이외에 실제적인 성공 사례는 아직 없습니다.</p>
<p>머신 러닝의 목표는 처음 본 데이터에서 잘 작동하는 일반화된 모델을 얻는 것입니다. 모델의 성능을 평가하기 위한 대표적인 세 가지 평가방법은 다음과 같습니다.</p>
<ul>
<li>단순 홀드아웃 검증 : 이것은 아주 기본적인 검증 방법으로 단순히 훈련 데이터와</li>
</ul>
<p>테스트 데이터로 나누고, 나눠진 훈련 데이터에서 다시 검증 데이터셋을</p>
<p>따로 떼어내는 방법입니다.</p>
<ul>
<li>K-겹 교차 검증 : 데이터를 동일한 크기를 가진 K개의 분할로 나눕니다, 각 분할 I에</li>
</ul>
<p>대해 남은 K-1 개의 분할로 모델을 훈련하고 분할 i에서 모델을 평가합니다.</p>
<p>최종 점수는 이렇게 얻은 K개의 점수를 평균합니다.</p>
<ul>
<li>셔플링을 사용한 반복 K-겹 교차 검증 : 이 방법은 K-겹 교차 검증을 여러 번</li>
</ul>
<p>적용하되 K개의 분할로 나누기 전에 매번 데이터를 무작위로 섞는 것입니다.</p>
<p>따라서 비용이 매우 많이 든다는 단점이 있습니다.</p>
<p>모델 평가 외에도 모델 개발로 들어가기 전에 데이터 전처리라는 중요한 과정이 있습니다. 데이터 전처리의 목적은 주어진 원본 데이터를 신경망에 적용하기 쉽도록 만드는 것입니다.</p>
<p>다음은 데이터 벡터화 방법 중 ‘원-핫 인코딩(One-hot encoding)’에 관한 것입니다.<br>
<img src="https://lh3.googleusercontent.com/I7BwJpNaC4PMdTKkkcHK6U0MPDFxUoWcmcZXWhLxssDI_lIvputAq46KRFTdDRITgiouQojtv3lc=s700" alt="" title="one-hot"><br>
이 외에도 값 정규화, 누락된 값 다루기, 특성 공학 등의 처리를 해줘야 합니다.</p>
<p>머신 러닝의 근본적인 이슈는 최적화와 일반화 사이의 줄다리기입니다. 과대 적합을 피하기위해 가장 단순하게 네트워크 크기를 축소 시킬 수 있습니다.<br>
<img src="https://lh3.googleusercontent.com/RM0OqiBKKsRtE2YY1Hx8nk50PmQtm9KfTtfC_RGfSGSl4eQU4M_KBMILX43GsQcXd2M616JmsML1=s700" alt="" title="base line"></p>
<p><img src="https://lh3.googleusercontent.com/cjrIHd3hSyklhY8J70Oc4-5x_lDfKldg2aI22Pqy1JGxnx5LYWdOr2a1vuOb5O7xHiatNtubx-0n=s700" alt="" title="baselinehistory"><br>
<img src="https://lh3.googleusercontent.com/YkLZK8xmtyVyXLfYmuK5qFn1hG6sajPgN0z-oZjJKX36eQvC7-BPZUoFsgO_bgqBmtzupkFl0QO3=s700" alt="" title="smaller"><br>
<img src="https://lh3.googleusercontent.com/JOZq8aKSls1Gsx5QOJRyEg_jmrgLNZEvFkV_Umeir4wyviSVFaix6i9o0MmdkTITJHx451KOduu8=s700" alt="" title="bigger"><br>
<img src="https://lh3.googleusercontent.com/qMkuLVmXuApWlac-rz2kEkDze2dNsokH5WsKN-le21rnPeo5D7JJ1v4gHtJWeW3y0-Y_wHKsXzX2=s700" alt="" title="plot-history"><br>
<img src="https://lh3.googleusercontent.com/kBMXu2r8a-hrHsiM-dO12mDk1fKTfqSqoP_0B4QRlYes0AOcTlUm75_Tbqt4u9FUYqqjsjIyWiLD=s700" alt="" title="plotting"></p>
<p>이 외에도 가중치를 규제하는 방법, 드롭 아웃을 추가하는 방법 등이 있습니다.</p>
<p><img src="https://lh3.googleusercontent.com/mOf9s-aE5jf0XkAaapzsioKBE4eMQSeTGNXjDEagMzn5WmcNrfCYlNZB3xlqm3sjOLH6Qu1KBh9V=s50" alt="" title="조원양"><strong>조원양</strong> :</p>
<p>
객체 탐지에 관련된 review
</p><p>객체 탐지(Object Detection)는 크게 두가지 방법으로 나눌 수 있습니다. 한가지 방법은 이단계(two stage) 방법입니다. 먼저 객체가 있을 것 같은 영역을 찾은 후 그 영역을 기반으로 객체를 탐지 합니다. R-CNN이라는 논문으로부터 시작이 되었습니다. 이 논문에서는 객체 탐지에서 CNN을 특성추출(Feature Extractor)로 사용하였습니다. 이 후에 이 논문을 기반으로 Fast R-CNN, Faster R-CNN 그리고 Mask R-CNN과 같은 방법이 등장합니다. 다른 한가지 방법은 일단계(one stage) 방법입니다. 이 방법의 특징은 보통 원본 이미지를 고정된 사이즈 그리드 영역으로 나누는데요. 알고리즘은 각 영역에 대해 형태와 크기가 미리 결정된 객체의 고정 개수를 예측합니다. SSD, YOLO, FPN(Feature Pyramid Network), RetinaNet등이 일단계(one stage)방법입니다. 두 개의 방법의 차이점은 정확도와 속도입니다. 정확도 면에서는 일단계(one stage)방법이, 속도면에서는 이단계(two stage)방법이 더 좋은 성능을 가집니다.</p>
<p>아트플로우에서는 이단계 (two stage) 방법인 Fast R-CNN, Faster R-CNN 그리고 Mask R-CNN과 일단계(one stage) 방법인 SSD, FPN(Feature Pyramid Network), RetianNet에 관련된 논문을 리뷰했습니다. 이단계(two stage)방법 중 하나인 R-CNN을 먼저 간략하게 살펴 보겠습니다.<br>
<img src="https://lh3.googleusercontent.com/8NFNWaM3MV-tQPAfe2Ur3vsITmD_C4KVmYa42zV0QVEqQUTzq6AtHxThflLBQBky15n-MLrs9zwI=s700" alt="" title="rcnn"></p>
<p>위의 그림에서 보듯이 R-CNN은 먼저 Selective Search로 먼저 객체가 있을 것 같은 영역을 찾습니다. 그리고 그 영역을 각각 CNN에 넣어서 이미지를 분류합니다. 그중에 가장 확률이 높은 영역에 대해 Bounding Box Regression으로 Box를 그립니다. 개념적으로는 단순하고 이해하기 쉽습니다. 그러나 Selective Search로 찾은 영역 각각에 대해 모두 CNN을 실행하기 때문에 상당히 느립니다.</p>
<p><img src="https://lh3.googleusercontent.com/cHGP_g0BEOP58XnxCjEhDqVwXJ62Gtxf6k0VYNTPEio7WrbXFpM3u-LZaxa8VkF_KkDSxk42aePr=s700" alt="" title="r-cnn2"></p>
<p>이런 단점을 극복한 것인 Fast R-CNN입니다. 후보 영역에 대해 CNN을 실행하는 것이 아니라 먼저 Selective Search로 영역에 대한 정보(x, y, w, h)만 구해놓고 CNN을 실행합니다. CNN을 거쳐 나온 특성맵(Feature map)에서 앞서 구한 정보를 투영시켜 영역을 구합니다. 이렇게 영역을 구하는 단계가 “ROI(Region of Interest) Pooling Layer”입니다. 이 각각의 영역을 다시 Fully Connected Layer에서 합한 후 분류(Classification)와 Bounding Box Regression을 합니다.</p>
<p>그런데 실제로 성능을 측정해 보면 객체 탐지(Object Detection)시 상당 시간을 Selective Search에 사용합니다. CNN은 GPU에서 구동이 되지만 Selective Search는 알고리즘 특성상 CPU에서 구동을 할 수 밖에 없습니다.</p>
<p>이런 단점을 보안 한 것이 Faster R-C Faster R-CNN은 Selective Search를 대신해서 CNN을 사용한 Region Proposal Network를 제안합니다.<br>
<img src="https://lh3.googleusercontent.com/vEVi9GrA2LLc9d-cc-lJs03x8rplIX4lMEJol5PhSMaCfnD_8TKqA9SNVOxNJ8fBpBVMRqs8L3b7=s700" alt="" title="r-cnn3"></p>
<p>CNN의 결과로 나온 특성 맵(Feature Map)을 Region Proposal Network에 넣어 객체가 있는지 없는지 여부와 Bounding Box Regression을 통해 Box를 어떻게 그려야하는지에 대한 결과를 받습니다. 이 결과와 특성 맵(Feature Map)에서 ROI Pooling을 통해 객체를 분류(Classification) 합니다. 추가적으로 Faster R-CNN에서는 Region Proposal Network에서 Anchor Box라는 개념을 도입했습니다. Region Proposal Network에서는 Sliding Window(보통 3X3 Convolution Layer)로 후보 영역을 찾습니다. 그리고 이때 사용하는 후보 Box를 미리 사전에 정의를 해서 쉽게 Bounding Box Regression을 할 수 있도록 합니다.<br>
<img src="https://lh3.googleusercontent.com/Smxn6tqM7lmCj8oCVi7AlxfMDU_HWz0rzWajGc3pTOSz5P5_U_CoYrrVoxPJLLXUD3Q8KRO9o1CF=s700" alt="" title="r-cnn4"></p>
<p>이제 또 다른 이단계(two stage)방법 Mask r-cnn을 살펴보겠습니다.</p>
<p><img src="https://lh3.googleusercontent.com/jkArkChfIMdrRwGHpQz-msVMRffZGFUdzlFCb2f4J2dCRPbP6_gQvPF5Cqq-gO2rDLH4FLPZ3fCc=s50" alt="" title="임하경"><strong><br>
임하경</strong> :</p>
<p>
</p><p><img src="https://lh3.googleusercontent.com/xC3j1O31UZEvVppuCm4j13oWek8-Od2KmVjxw6XnCJQHBPNKNULXgVmcVh4jyLnk63hManKSE-Df=s700" alt="" title="r-cnn"></p>
<p>기존의 Faster R-CNN은 image에서 convolution layer를 통해 feature map을 뽑아내고 Region Proposal Network를 통해 bounding box를 뽑아냈습니다. 이 새로 개발된 Region Proposal Network를 통해 매우 빠른 속도로 bounding box를 뽑아낼 수 있었기 때문에 faster R=CNN은 fast R-CNN보다 월등히 빨랐습니다.</p>
<p>Mask R-CNN은 이 Faster R-CNN에 FCN을 추가한 모델입니다. FCN이란 Fully conncected neural network의 약자이며 sementic segmentation을 위해 개발된 모델이었습니다. Faster R-CNN을 비롯한 기존의 image detection 모델은 bounding box를 통해 object를 detection하고 classification을 통해 그 object의 label이 무엇인지 파악하는 구조였습니다. 즉 detection과 classification이 모델을 이루는 두 뼈대였고 이 두 가지 소스를 통해 모델의 가중치가 학습되었습니다. 그런데 Faster R-CNN은 여기에 segmentation이라는 모델을 하나도 추가해 준 것입니다. 따라서 이 모델은 두 가지가 아닌 세 가지 소스로 가중치가 학습되었고 따라서 정확도가 기존의 모습보다 더 향상되지 않을 수 없었습니다. 그리고 이 방법은 사람이 사물을 인식하는 것과도 더 닮았습니다. 우리도 사물을 볼 때 그 사물의 위치를 잡을 뿐만 아니라 그 사물을 segementation하여 정확한 모양을 파악하니까요.</p>

이제는 일단계(one stage) 방법들인 SSD, FPN, Retinanet을 차례대로 살펴보겠습니다.
<p><img src="https://lh3.googleusercontent.com/6E48N4HzEvzux8H5qd9jFC96tkEKqXH7DBg8gnAtfoe5E-0FU2PZgjO5kmEJyY9xZOabPUrs28ME=s50" alt="" title="전민종"><strong>전민종</strong> :</p>
<p>
SSD 같은 경우에는 우선 trade-off 문제를 해결하기 위해서 고안되었습니다. Trade-off 문제란, 속도를 향상시키게 되면 정확도가 떨어지고, 정확도를 향상시키면 속도가 떨어지는 현상입니다. 이런 trade-off문제를 해결하기 위해 고안된 ssd의 가장 큰 특징 2가지는 single shot detector 와 multi scale feature maps for detection입니다.
</p><p>우선 ssd의 구조를 살펴 보게 되면, ssd는 크게 3가지의 구조로 나눌 수 있습니다. 첫번째로는 이미지를 입력 받고, 두번째 단계에서는 특징을 추출 및 분류를 하며 마지막으로는 축적된 특징들을 통해서 Non-Maximum Suppression과 비용함수 설정을 통해서 값을 산출하고 피드백을 주는 구조입니다.<br>
<img src="https://lh3.googleusercontent.com/kImF32Fy7HZjWqJUdIiUsrHSD-f3PYBPImlhwMj_aNkH-amK1wDDn8UZf-9LavlPO76f_OWecuGb=s700" alt="" title="ssd"></p>
<p>첫번 째 특징으로 Single Shot Detector 관련해서 말하면, Fast RCNN과 RFCN 같은 Dector들 같은 경우에는 2-Stage Detector입니다. 2-Stage Detector를 간단하게 말하자면, Region proposal과 classfication이 차례대로 일어납니다. 따라서 한번 추출을 하고, bounding box를 조정하여 다시 특징을 추출하는 작업이 반복적으로 일어나게 됩니다. 반면에 1-Stage Detector 같은 경우에는 Region Proposal과 classification이 동시에 일어나기 때문에 bounding box를 계속 조정하는 반복을 하지 않아도 되는 장점이 있습니다.</p>
<p>두번 째 특징으로는 multi scale feature maps for detection입니다. 위에서 말한 Region Proposal과 classification을 동시에 하게 해주는 방법입니다. 이러한 특징의 가장 기본 원리는 bounding box 크기는 그대로 두고 입력 받은 이미지를 개념적으로 축소 한다는 것입니다. 이렇게 개념적으로 축소를 하게되면 처음에 발견하지 못했던 특징들도 축소를 통해 점점 자세히 볼 수 있게 되면서 찾을 수 있게 됩니다. 특히 밑에 사진을 보면 기존의 Detector로는 같은 이미지 내에서 사물의 크기가 너무 상이할 경우에는 서로 다른 boundig box를 적용해야 돼서 시간이 더 걸렸지만 ssd는 이를 해결하게 해줍니다. 이렇게 여러개의 feature들을 추출 한 후 축적하여 nms를 통해서 최종적으로 분류 및 피드백을 제공합니다.<br>
<img src="https://lh3.googleusercontent.com/cKicxl77K8kKXvCklM_YwjE1bzFispQR9Y8Y4iYVXb9MNRaVbHxVShSTuWKq2V_uOuOAfFtnrzDL=s700" alt="" title="dogs"></p>

<p><img src="https://lh3.googleusercontent.com/RICyunnCjhRK8WYrUhJQkeFJ2uk0aNgr_wR7LiD_vUnFmSALHqJPHVfKdtNhgF6_xHeBd9heeZf6=s50" alt="" title="임한동"><strong>임한동</strong> :</p>
<p>FPN은 multi scale object detection의 accuracy를 향상시키기 위해 나온 network 입니다. 우선FPN에 활용되는 Image Pyramid 라는 개념에 관해 먼저 이야기를 하겠습니다. Image pyramid은 고전적인 컴퓨터 비전 영역에서 많이 사용되었는데, 일정 pixel 간격으로 subsampling 해서 image layer, 즉 size가 다른 image 층들을 만드는 것입니다. 피라미드 상단은 low level feature는 없어지고 high level feature만 남아서 semantic value가 커지게 되지만 해상도는 줄어들게 됩니다. 이렇게 사이즈가 다른 이미지들을 활용해 FPN은 Multi Scale Object Detection에서 좋은 성능을 보입니다.
</p><p>물론, 기본적인 CNN 구조에 위에서 설명한 Image Pyramid을 활용할 수도 있지만 시간이 너무 오래 걸리다는 문제점이 발생합니다. 이러한 문제점을 해결하기 위해 SSD가 나왔습니다. 하지만 SSD의 겨우, feature map마다 semantic gap이 크고 처음에 나오는 맨 밑단의 feature map들은 semantic value가 낮은, 즉, low level feature들로 인해 여전히 scale variant object를 잘 detect하지 못하는 문제점이 발생합니다. 그래서 이러한 문제를 해결한 것이 FPN입니다.</p>
<p>FPN의 전체적인 개념은 상단의 semantic value는 크고 해상도는 낮은 feature map들과 하단의 semantic value는 낮지만 해상도가 높은 feature map들을 결합해서 모든 feature map level에서 rich한 semantic value를 가질 수 있도록 만들어보자는 것입니다.<br>
<img src="https://lh3.googleusercontent.com/wf5X-8wIj0Wpn7H0il3U9ewEK9qzey79Blnn3xSt5Q2SYM4Bb4v7xetdKuKyPOoTVQnkFyk2LZNv=s700" alt="" title="fpn"></p>
<p>FPN은 크게 Bottom-Up, Top-Dpwn, Later Connection 세 가지 구조로 이루어져 있고 Backbone network로 Resnet을 사용하고 있고 여기서 나오는 feature map들을 이용하고 있습니다.</p>
<p>Top down구조는 semantic gap문제를 해결해주고 lateral connection은 location accuracy 문제를 해결해준다고 논문에서 설명하고 있습니다. 이렇게 생성된 feature map들을 Faster RCNN과 결합해 좋은 성능을 낼 수 있습니다.</p>
<p>&lt;![if !supportEmptyParas]&gt; &lt;![endif]&gt;</p>
<p>Tsung-Yi Lin1,2, Piotr Doll´ ar1, Ross Girshick1, Kaiming He1, Bharath Hariharan1, and Serge Belongie</p>
<p>[Feature Pyramid Networks for Object Detection]</p>
<p><img src="https://lh3.googleusercontent.com/mOf9s-aE5jf0XkAaapzsioKBE4eMQSeTGNXjDEagMzn5WmcNrfCYlNZB3xlqm3sjOLH6Qu1KBh9V=s50" alt="" title="조원양"><strong>조원양</strong> :</p>
<p>
RetinaNet
</p><p>Retina Net은 일단계 (one stage)의 정확도가 떨어지는 것이 “Forground와 Background Class의 불균형”으로 생각했습니다. 즉, 배경과 같이 쉽게 탐지가 가능한 것은 가중치를 낮추고 객체와 같이 배경에 비해 탐지가 어려운 것은 가중치를 높인 Focal Loss라는 손실함수를 사용합니다. SSD에서도 이런 문제를 해결하기 위해 Hard Negative Mining이라는 방법을 사용하였고 YOLO계열에서는 Confidence Score 라는 것을 사용했지만 Focal Loss가 상대적으로 사용하기가 쉽습니다.<br>
<img src="https://lh3.googleusercontent.com/8vKQfWR38zvx0cgtE1JyPS2g3Qinzvum2ho5HzhaJeuTZ8Hv8GPhufxYYwQ2JmI5PwyDbwg3RTEh=s700" alt="" title="RETINA GRAPH"></p>
<p>RetinaNet은 이 Focal Loss와 ResNet에 Feature Pyramid Network를 살짝 변형하여 사용한 네트워크입니다.<br>
<img src="https://lh3.googleusercontent.com/TlgMv11pBEv5it47SWvm6WFwfWES6AzCTI76qv0QAc3n-pD7P9TK14nvW-lfG1RP1LcQbDVvy1oq=s700" alt="" title="retinanet"></p>
<p>그리고 추가적으로 강화학습 관련된 논문도 리뷰했습니다.</p>
<p><img src="https://lh3.googleusercontent.com/gl7bkkPmOIthPt1z_fA9jCx4whqnUwIaI23ItYk1ygz3JAxn7av5soLWa_JU9oLoTKcv4OIiIuMC=s50" alt="" title="민예린"> <strong>민예린</strong> :</p>
<center>Reinforcement Learning for Recommender System</center>
<p>0 Introduction</p>
<p>구글의 YouTube 추천 시스템은 Collaborative filtering, DNN, reinforcement learning 순으로 발전 해왔다. 따라서 본 보고서는 기존에 사용되던 추천 시스템과 이들의 한계를 극복하기 위한 강화 학습 적용에 대해 서술할  것이다.</p>
<p>
Section 1: Collaborative  filtering
</p><p>Section 2: DNN for YouTube  recommendation</p>
<p>Section 3: Deep reinforcement learning based  recommendation</p>
<p>Section 4: Deep reinforcement learning  for YouTube recommendation</p>
<p>Section 5:  Conclusion</p>
<p>Section 1: Collaborative filtering(이하 CF)</p>
<p>CF는 ‘유유상종( 類 類 相 從 )’을 가정으로 하여 만들어진 기술로 추천 시스템의 가장 기본적이면서도 핵심이라고 할 수 있다.</p>
<p>User-based CF: 유사한 성향을 가진 users를 구분하고 그 users들이 좋아하는 것을 이용해 추 천하는  방식</p>
<p>Item-based CF: 각 user가 이전에 구매했던 item을 이용해 유사한 물건을 추전하는 방식</p>
<p>위와 같은 방법은 model-based 방법이며 모델을 만드는데 사용되는 대표적인 방법으로는 matrix factorization이 있다.</p>
<p>Matrix factorization: user와 item을 각 행과 열로 하는 matrix를 만든다. 이는 모든 user가 모 든 item을 소비할 수 없다는 가정하에 matrix의 비어 있는 부분을 채우는  기술이다.</p>
<p>현재 아마존에서는 CF를 이용해 상품을 추천하고 있으며, large scale issue를 위해 CF와 neural attention mechanism을 결합하여 이용한다.</p>
<p>Attention mechanism: 모든 context에 같은 가중치를 부여하는 것이 아니라 중요한 것에 집 중해 가중치를 부여하는  메커니즘</p>
<p>Section 2: DNN for YouTube recommendation
</p><p>2.1 Challenges with recommendation  system</p>
<p>Scale:  데이터의  크기가  크기  때문에  기존  알고리즘으로는  작동하기  어렵다.</p>
<p>Freshness: 새로운 비디오가 실시간으로 올라오며, 이것이 바로 추천에 반영될 수 있어야 한 다.</p>
<p>Noise: user 만족도를 측정하는 근거가 거의 implicit feedback이다. 예를 들어 관심이 없는<br>
분야라도  실수로  동영상을  시청할  수  있는  경우가  존재한다.  따라서  feedback에  noise가<br>
많다.</p><br>
2.2 System  overview<br>
<img src="https://lh3.googleusercontent.com/XuKBdv915lJ_bJAkZD4wcQa5axFJyukH_89jnm70aqVls6eZBcfPpYkIJ8UWfTPPXYq5H0Oa8qdM=s700" alt="" title="1.">
<ul>
<li>
<p>Video corpus: user의 YouTube 사용 기록 corpus를 embedding vector로 변환하여<br>
input으로   사용한다.</p>
</li>
<li>
<p>Candidate  generation:  user에게  추천할  후보  동영상을  몇  백  개  선정한다.</p>
</li>
<li>
<p>효율적인 구현을 위하여 precision, recall, ranking loss 등의 과거 기록은 offline 측정을<br>
이용한다.</p>
</li>
<li>
<p>Freshness를 위해 동영상의 나이를 input으로  넣어준다.</p>
</li>
<li>
<p>동영상을 굉장히 많이 보는 outliers의 영향을 빼기 위해 user별 영상 감상 횟수를 제한한 다.</p>
</li>
<li>
<p>검색 키워드에 대한 corpus는 순서를 날린 bag-of-tokens를 이용한다.</p>
<p>(방금 검색한 내용 이 계속 메인 페이지에 뜨는 것을  방지하기 위해)</p>
</li>
<li>
<p>Ranking: 선정된 후보들을 순위 매겨 output으로  추천한다.</p>
</li>
</ul>
<p><img src="https://lh3.googleusercontent.com/qsAsStMM7yZaCqN3GrYkNaF4lcB0bfc13cS6RTU6VCQxJZYziIbYI7zi081d-B-8oKfEo9uOVIv3=s700" alt="" title="2"></p>
<ul>
<li>기존 feature뿐만 아니라 추가한 feature를  사용한다.</li>
<li>대략 수백 개 정도의 feature를  사용한다.</li>
</ul>
<p>Section 3: Deep reinforcement learning based recommendation</p>
<p>3.1 Background</p>
<p>구글의 강화 학습 적용 사례를 보기에 앞서, 논문 ‘Deep Reinforcement Learning based Recommendation  with  Explicit  User-Item  Interactions  Modeling’을  통해  강화  학습이  추천  시스템에 적용되는 컨셉을 이해해 보고자  한다.</p>
<p>본 논문에서는 이전의 추천 시스템에서의 두 가지 한계에 대해 얘기하는데, 이는 구글에서 말하는 이전 추천 시스템의 한계와 같다. 이러한 한계들을 극복하기 위해 ‘novel recommendation framework  based  on  deep  reinforcement  learning’  (이하  DRR)의  적용이  필요하다.</p>
<p>User의  preference가  계속  변하지  않는다고  가정한다.  하지만  user의  preference는  dynamic<br>
하므로 이 가정에는 문제가 있다.</p>
<p>직전의 reward를 최대화하는 것에 집중하며, 추천한 items이 click 또는 consume 됐는지 여 부에만 집중한다. 하지만 직전 reward에만 신경 쓰는 것은 long-term contributions를 무시할 가능성이  있다.</p>
<p>본 논문에서는 ‘actor-critic’ reinforcement learning을 이용할 예정이며, 이는 user와 recommender systems 사이의 상호작용을 모델링하는 것에 활용된다.</p>
<p>3.2 Preliminaries<br>
<img src="https://lh3.googleusercontent.com/vX0q-OGXKLzimqOOc57QU0Kp80w-kB1JvLdkQIJdOuy21I7FF6NIahggdxRN7pwK0lZsdV305fZF=s700" alt="enter image description here" title="2"><br>
-State S: user’s positive interaction history</p>
<p>with recommender.</p>
<p>-Actions  A:  continuous  parameter  vector.  (모 든 candidate items의 ranking scores를 결정, Top-N items이  추천됨)</p>
<p>-Transitions P: state는 user의 긍정적 상호작용 기록의 representation이다.<br>
그러므로 한번 user의 feedback이 들어오면 state transition 이 determine된다.</p>
<p>-Reward R: user’s feedback(satisfaction).</p>
<p>-Discount rate γ : factor measuring the present value of long-term rewards.<br>
<img src="https://lh3.googleusercontent.com/tDp93c5HyrzK04VetAJsQTXTI5MjWuW_0eMMccGdMmQJWtKc5VgRA9KFbJ4C0E9WBclSaS-3WvJ_=s700" alt="" title="3"><br>
Actor-critic  기반  DRR은  3가지  필수  요소가  있다.</p>
<p>1)  Actor  network</p>
<ul>
<li>Main  network로  policy  network라고도  한다.</li>
<li>User state를 input으로  간주한다.</li>
<li>Embedding한 input은 user에 대한 요약된 representation s로  공급된다.</li>
<li>2개의 ReLU layers와 하나의 Tanh layer를 이용해 representation s를 actor network의<br>
output인 action a로 transform 시킨다. (기존 DNN은 3개의 ReLU layers)</li>
<li>Top  ranked  items이  users에게  추천된다.</li>
</ul>
<p>2) Critic  network</p>
<ul>
<li>DQN의  network이다.</li>
<li>User  state  representation  module과  actor  network의  action  a로<br>
generate된 user state를 input으로  간주한다.</li>
<li>Output은 Q-value이며  scalar이다.</li>
<li>Q-value에 따라 actor network의 parameters가 업데이트 되며, 이는 action의 성능을<br>
향상시킨다.</li>
<li>Policy  update에  관련한  식은  아래와  같다.<br>
<img src="https://lh3.googleusercontent.com/-Zyi1Y3tI7yPoMT1_2r5862fOxt9jcvgJsKifTHZldcmWE5HN7r8egF0bd3VI8EbOPQbSNZmCkyh=s700" alt="" title="4"><br>
<img src="https://lh3.googleusercontent.com/v7E-AhDHcg1U8k-YIKNFkh7GARwsU7qYpiunBy-1xf_Y_KB1AgatCgALYEIqvvm9b8aZPhZJpsTa=s700" alt="" title="5"><br>
첨언하자면, Q는 actor network의 output인 action a와 state를 input으로 사 용하는 critic network에 대한 함수이다. 또한 본 논문에서는 DDPG를 이용했 기 때문에 DDPG의 loss function과  같다.</li>
</ul>
<ol start="3">
<li>State representation: actor network와 critic network에서 중요한 역할을 한다. 이것은 state를 모델링 하는 데에 필수적이며, features간 상호작용을 명확하게 모델링 하면 추천 시스템의 성능을 향상시킬 수  있다.</li>
</ol>
<p>3.4 DRR-ave<br>
state를 모델링하는 structure로 평균을 이용한다.<br>
<img src="https://lh3.googleusercontent.com/S1qV7bXyWjjU0uDMBdht_bGRNL0zXidzjly_Bq5WiMhj_IJGL6JwAxzxzMex6-VvHbwv_R4OeN9l=s700" alt="" title="5"></p>
<ol>
<li>H를 weighted average pooling layer에 의해  변형시킨다.</li>
<li>변형시킨  vector는  input  user와  상호작용을  모델링  하는  것에  사용된다.</li>
<li>User의 embedding과 items의 average pooling layer를 합쳐 state<br>
representation을 만든다.</li>
</ol>
<p>H:  n개의  item들로  이루어진  집합  {𝑖1,  𝑖2,  …  ,  𝑖𝑛}</p>
<p>3.5 Algorithm<br>
<img src="https://lh3.googleusercontent.com/4dmAfM15Lxb073DfDzEA2-YY5piBilQkWhxTIWBhZ1M5lrKUnwe9Z1R1LI_mMj9Rg3_10O2mE4ui=s700" alt="" title="6"></p>
<ul>
<li>한 에피소드(session) 당 들 어있는 time만큼 학습</li>
<li>H에 대한 state를  관찰하고</li>
<li>action의 policy 찾음</li>
<li>Item을 추천하고 user의 feedback으로 reward  계산</li>
<li>다음 time state에 영향을 준다.</li>
<li>Critic policy와 actor  policy를 구한다.<br>
(main policy는 actor policy)</li>
<li>Policy  update</li>
<li>모든 에피소드가  끝나면<br>
policy와 weight return.</li>
</ul>
<p>Section 4: Deep reinforcement learning for YouTube recommendation<br>
4.1 Preliminaries</p>
<ul>
<li>Agent: Candidate generator (use data source to build  agent)</li>
<li>State: user interest,  context</li>
<li>Reward: user satisfaction</li>
<li>Action: nominate from a catalog of millions of  videos</li>
</ul>
<p>4.2 Challenges of the RL based recommendation system and what google does for  these</p>
<ol>
<li>Large action  space</li>
</ol>
<ul>
<li>Action  space가  너무  크면  long-term  reward를  최대화하기  어렵다.</li>
<li>따라서 policy-based RL을  이용한다.</li>
<li>이는 value-based와 비교했을 때 더  안정적이다.</li>
</ul>
<p><img src="https://lh3.googleusercontent.com/4uTSWr4Mg177RhV-9Po4gC8HtuktodEmrAEqu6WzkWufBSNBkkvwmpcjQL7MYC3aTY7t9AEf7zM4=s700" alt="" title="6"><br>
where T is a temperature that is normally set to 1.</p>
<ol start="2">
<li>
<p>Expensive  exploration</p>
</li>
<li>
<p>Learning  off-policy</p>
</li>
</ol>
<ul>
<li>Agent가 5시간마다  refresh된다.</li>
<li>이것은 5시간 뒤의 policy와 지금의 policy가 매우 다르다는 것을  의미한다.</li>
<li>따라서 off-policy를 이용한다. (Otherwise, traffic  발생한다.)<br>
<img src="https://lh3.googleusercontent.com/oqBHcBVRLtx4X3mWJ_80ZfaeLyEwjtCSPEOry6yQgYG7jk8e9fkTAaWgXkcKmPviJPUHVSF75R3N=s700" alt="" title="7"></li>
<li>Trajectory  𝜏  =  (𝑠0,  𝐴0,  𝑠1,  …  )</li>
<li>𝛽는  behavior  policy이다.  전체  corpus에서  action  빈도를  집계하여  user<br>
state와  독립적으 로 추정된 𝛽를 구할  수 있다.</li>
</ul>
<ol start="4">
<li>Partial  observability</li>
</ol>
<ul>
<li>User들은  그들의  interest와  추천  내용에  대한  직접적  feedback을  말하지  않는다.</li>
<li>따라서  partial  observability라는  challenge가  발생하게  되는데,  이때  RNN을<br>
이용한다.<br>
<img src="https://lh3.googleusercontent.com/QtI3t4cDalnQG0PZhaW2h97s5irl4D_KayRrKy3OR3GaFpENFhK6MW1WmeD75BmXJPhlbdOHp6Rm=s700" alt="" title="8"></li>
<li>아래는 RNN안에서 state가 update되는  형태이다.<br>
<img src="https://lh3.googleusercontent.com/bK83Gt2RZpKnzVZkhQqaGcaFS7SQ-rBwrFRcWOvVf67nPP9A02h3Ejd5Ges9BMHLEm-neRNEJUPp=s700" alt="" title="9"><br>
where 𝑧𝑡 , 𝑖𝑡 ∈ ℝ𝑛  is another embedding for each action a in the action space.</li>
<li>User가 추천을 받기 전에 보았던 동영상들을 RNN에  넣고 최종 state를 결정한다.</li>
<li>또한 동영상 기록 이외에도 device type과 같은 것들이 user의 interest에 영향을 준다. 따 라서 추가로 features를  넣어준다.<br>
<img src="https://lh3.googleusercontent.com/gHRYvQdNtvJr7KbGBUYBRm3_wNz7Q-f67wk3cBJ_obAzktnOwiTK_gqKTYBcEus5vFznyvRzShW1=s700" alt="" title="10"></li>
</ul>
<ol start="5">
<li>Noisy  reward</li>
</ol>
<ul>
<li>User  feedback의  noise를  다루기  위해서는  future  reward를  discount하여  모아야<br>
한다.<br>
<img src="https://lh3.googleusercontent.com/_0QCBbxZ94l7uDYRWFNYyOG-EjFxVc5rSkjb91arBBc4LIlgaT3YG_itFrLqqQ9edLk0dj-TEXQQ=s700" alt="" title="11"></li>
<li>Discounted future reward의 컨셉은 강화 학습에서의 discounted factor와  비슷하다.</li>
</ul>
<p>Section 5:  Conclusion</p>
<ul>
<li>강화  학습은  actions의  complex  sequence를  state로  바꾸고  long-term<br>
reward를  최대화하는 것에  이용된다.</li>
<li>Embedding에 BERT를 적용한 사례도 확인해 보았는데, embedding 부분에 어느 방법을 적용 할지 고민할 필요가 있다.</li>
<li>구글도  cold  start  issue에  대해서  고민  중이다.</li>
<li>Cold start issue: 기존에 없던 새로운 user가 등장했을 때, 추천할 정보가  부족하다.</li>
<li>구글의 future work는  이러하다.
<ul>
<li>Better state representation through LRD</li>
<li>Better exploration and  planning</li>
<li>Beyond systems and users: improve YouTube  ecosystem.</li>
</ul>
</li>
</ul>
<h3>13. 랩원들의 후기를 부탁합니다.</h3>
<p><img src="https://lh3.googleusercontent.com/1kd62PD4v3zemne3ezoOCYS47e8xULOOK_fyHTxQChCxb7hd2RcWbhDr_Bh2GxOKfVe-4ai4bT6Z=s50" alt="" title="김훈민"><strong>김훈민</strong> :</p>
<p>처음 딥러닝을 접한 후 혼자서 독학을 시작하고 답답한 마음이 많았어요. 무엇을 어떻게 공부해야 할지 잘 모르는 상태였거든요. 딥러닝 기초 책을 보며 예제 코드를 따라 하고 이해하는 수준이었죠. 그러던 중에 좋은 기회가 있어 ‘한국인공지능연구소’에 참여를 하게 되었고, 많은 새로운 정보들을 얻고 여러 가지 경험을 해볼 기회가 되었다고 생각해요. 특히 여러 논문 리뷰를 하며 여러 모델 구조를 이해하는 능력을 많이 키웠다고 생각해요. 제가 연구원으로 활동하며 얻은 것 중 가장 큰 결과랍니다. 항상 많은 정보를 공유해주시는 아트플로우 랩장님과 랩원들께 감사하게 생각하고 있어요! 앞으로도 아트플로우 화이팅입니다~!</p>
<p><img src="https://lh3.googleusercontent.com/JCElC-S7Oh8ERUkNxE99JqpL7bP7_naNibkPHEaSnIMhUCbIwrlM_YToiWoDfb5YKPOykktOJ1-r=s50" alt="" title="박성은"><strong>박성은</strong> :</p>
<p>분위기가 너무 무겁지도, 너무 가볍지도 않은 자유롭게 학문에 대한 의논을 나누며 공부하는 모습이 너무 좋아요. 그만큼 좋은 랩을 만났다는 의미인거겠죠.
</p><p>또 각자 전공이 달라서 다양한 시각에서 바라보는 AI가 새롭게 또는 즐겁게 느껴지곤 해요. 이렇게 계속 나아가 각자 원하는 AI에 대한 목표를 이루었으면 좋겠고 한국인공지능연구소에게도 이런 연(緣)을 만나게 해주어 너무 감사합니다.</p>
<p><img src="https://lh3.googleusercontent.com/EEh6AtQW8AOdzrf_JzB1bXZ9gyHkUlXc__MyiTId_281HuMyQvmqZKGfW6_T1hPsBgYQ0rtv3e0k=s50" alt="" title="윤현근"><strong>윤현근</strong> :</p>
<p>
4기활동을 마무리하게 되었네요. 4기부터 합류하게 되어 어색하기도 하였고 어떻게 해야 할지도 모르고 있었는데 다른 분들이 발표를 준비한 것을 보고 깜짝 놀랐습니다. 모두 자신이 맡은 발표 준비를 열심히 하였고 자료에도 많은 노력을 기울인 것을 알 수 있었습니다. 바쁘다는 핑계로 제가 직접 만들지도 않고 가지고 있던 자료를 가지고 왔던 제가 부끄러웠습니다. 그래서 다음에는 좀 더 많이 준비하고 많은 것을 공유할 수 있도록 노력해야겠다는 생각을 하게 되었습니다.
</p><p>4기 막바지에야 가진 처음이자 마지막이 되어버린 4기의 뒤풀이에서 사실 모두가 친해지길 원했고 좀 더 자주 그리고 좀 더 많은 활동을 원했다는 것이 재미있기도 했지만 제가 좀 더 적극적으로 이야기해볼걸 그러지 못한 소심함에 아쉬움이 남기도 하였습니다. 하지만 모두의 마음을 알 수 있었고 그래서 5기를 더 기대 할 수 있게 되었습니다.</p>
<p>전 순수한 열정을 가진 다양한 사람들과 공부하고 싶다는 마음으로 ‘한국인공지능연구소’를 찾아왔습니다. 이번 4기는 워밍업을 하는 시간으로 충분하였다는 생각이 듭니다. 5기에서는 더 즐겁게 공부하고 추억을 쌓을 수 있는 소중한 시간을 함께 하길 바라고 또 그렇게 될 것이라고 기대하고 있습니다.</p>
<p><img src="https://lh3.googleusercontent.com/KuG8ITu2uYEPzkFN8heXISyqcljp073gjwTBxX7L_KsLPUv_LkGyP2mwmVubFu_NlfQT21Nh2j1P=s50" alt="" title="이준석"><strong>이준석</strong> :</p>
<p>처음에는 ‘혼자 뒤쳐지면 어떡하지?’라는 두려움이 있었습니다. 하지만, 랩원들이 너무 친절하셨고 랩장님도 너무 좋으셔서 그런 걱정은 금방 사라졌습니다. 돌아가면서 발표를 하면서 얻어가는 것도 너무 많았고, 정말 편안한 분위기에서 공부를 할 수 있어서 너무 좋았습니다. 제일 좋은 것은 너무 노는 분위기도 아니고, 그렇다고 딱딱한 분위기도 아닌 편안한 분위기에서 공부할 수 있는 것입니다. 앞으로도 계속 아트플로우 랩실에서 열심히 공부할 것이고, 좋은 분들과 함께 하게 되어서 매우 뿌듯합니다.</p>
<p><img src="https://lh3.googleusercontent.com/7lsIIljE5rgp8KzskVHL7LORoL2Bb4u-PjVSwGzgf1I7h-4fHWknz64e5pNsQJIB6nr8DqZ3bqGS=s50" alt="" title="이지은"><strong>이지은</strong> :</p>
<p>벌써 4기 활동이 끝났나 싶을 정도로 빠르게 지나갔던 것 같습니다. 다양한 분야와 연령대의 사람들이 모인 만큼 어디에서도 들을 수 없는 이야기들을 듣는 것이 즐거웠고, 많은 인원이 모였음에도 다들 열심히 발표 준비를 하셔서 저도 더 열심히 임하게 되었던 것 같습니다. 브이로그, 책 집필 등 새로운 시도들을 해볼 계획이고, 회식 이후로 친밀도도 더욱 급상승해서 앞으로의 활동이 무척이나 기대됩니다.</p>
<p><img src="https://lh3.googleusercontent.com/jkArkChfIMdrRwGHpQz-msVMRffZGFUdzlFCb2f4J2dCRPbP6_gQvPF5Cqq-gO2rDLH4FLPZ3fCc=s50" alt="" title="임하경"><strong>임하경</strong> :</p>
<p>두 기수 동안 랩 활동을 진행하면서 이 분야는 정말 어렵다고 느꼈습니다. 처음 시작할땐 나도 열심히 하면 모델도 새로 개발하고 더 낫게 수정도 할 수 있을 것만 같았는데 공부를 시작해보니 모델의 종류도 너무나 많고 돌아가는 원리를 이해하는 것만으로도 벅차더군요. 그래도 계속 진행할 수 있었던 것 좋은 랩원 분들이 있어서였던 것 같습니다. 공부가 부족하더라도 서로 이해해주고 부담없는 환경에서 편안하게 랩활동을 할 수 있었기 때문에 6개월 동안 꾸준하게 할 수 있었던 것 같습니다. 앞으로도 이렇게 좋은 분들이랑 즐겁게 랩활동 할수 있기를 바랍니다~</p>
<p><img src="https://lh3.googleusercontent.com/RICyunnCjhRK8WYrUhJQkeFJ2uk0aNgr_wR7LiD_vUnFmSALHqJPHVfKdtNhgF6_xHeBd9heeZf6=s50" alt="" title="임한동"><strong>임한동</strong> :</p>
<p>아트플로우 활동은 2주 간격으로 local minima에 빠진 저를 구원해주는 그런 존재였습니다. 매 활동마다 많이 배워가고 새로운 인사이트를 얻을 수 있었습니다. 결과적으로 3개월이라는 짧은 시간 동안 많이 발전할 수 있었던 것 같습니다. 랩장님이 잘 지도해주시고 특히 다 같이 참여하는 분위기가 형성되어 랩 구성원들 모두가 같이 성장해 나갈 수 있었습니다. 마지막에는 랩 구성원들 모두가 친해져 다음 기수 랩 활동도 아주 학수고대하고 있습니다.</p>
<p><img src="https://lh3.googleusercontent.com/6E48N4HzEvzux8H5qd9jFC96tkEKqXH7DBg8gnAtfoe5E-0FU2PZgjO5kmEJyY9xZOabPUrs28ME=s50" alt="" title="전민종"><strong>전민종</strong> :</p>
<p>솔직히 처음 한국인공지능연구소 데모 데이 때 가서 다른 랩들의 발표를 들으면서 지레 겁을 먹고 집에 다시 돌아갈까 수십 번은 고민했습니다. 또 함께 듣는 사람들은 발표를 들으면서도 다들 코딩하고 있고 있더라고요. ‘아 이게 진짜 개발자들인가…’라는 생각이 들어 정신이 다 나간 기분이였습니다. 결국 마지막까지도 어떤 랩에도 들어가지 못하고 방황하다 같은 처지인 사람들끼리 모였었는데 벌써 6개월이 지났네요. 인원이 이렇게 많은데도 이탈 없이 잘 진행되는 건 다 훌륭한 랩장님과 랩원들 덕분인 것 같습니다(아 물론 무엇보다도 제가 굉장히 인격적으로도 훌륭하고 뛰어난 사람이기에 가능했다고 믿어 의심치 않고요.). 항상 너무 즐거운 분위기와 에너지를 내는 랩원들 모두 애정하구요~ 앞으로도 계속 같이 연구할 수 있으면 좋겠습니다!</p>
<p><img src="https://lh3.googleusercontent.com/mOf9s-aE5jf0XkAaapzsioKBE4eMQSeTGNXjDEagMzn5WmcNrfCYlNZB3xlqm3sjOLH6Qu1KBh9V=s50" alt="" title="조원양"><strong>조원양</strong> :</p>
<p>처음에는 과연 아트플로우 랩이 얼마나 갈 것인가에 대해 걱정을 많이 했습니다. 파이썬을 설치 안 해본 연구원들도 많았고 텐서플로우나 파이토치가 뭔지도 모르는 연구원들도 많았습니다. 하지만 매 기수 연구원들이 수가 늘고 참여하는 연구원들의 실력이 향상되는 것을 보면 참여하기를 잘했다는 생각이 듭니다. 서로 끌어주고 밀어주면서 좋은 사람들이 모여서 즐겁게 연구를 할 수 있다는 것 자체만으로도 행복합니다.</p>
<h3>Artflow 사진들</h3>
<p><img src="https://lh3.googleusercontent.com/lsuHRJU7iT4kmdzbrRr68_qnvMVXoCePcnB-cwg1H7DM7F6kFKw0fuIuQeTLtxLGr8kZ1mtWTPYn=s500" alt="" title="artflow1"></p>
<p><img src="https://lh3.googleusercontent.com/6zXCa2myDVNumh7e3OLawS8nqZIP9e9tUlXXNukg7lncJ2CjT0o_JW_E_KpsKy2YR-q6_aYUHIgw=s500" alt="" title="artflow2"></p>
<p><img src="https://lh3.googleusercontent.com/crLFxEPZf7-YisZtwWKokxhpVKPjE_0u0xP7WuHT6uSoCGuXlZlS3JyA1VOM0inTKwxkwEeoEJgg=s500" alt="" title="artflow3"></p>
<p><img src="https://lh3.googleusercontent.com/PrQKAF1tpf1unzAp1Ns7g88p61AvcXwg5hB8Y5Mo6fidIhAPXp-3scvCoy4JwUGV2_jmyjm7Z9Wb=s500" alt="" title="artflow4"></p>
<p><img src="https://lh3.googleusercontent.com/ZFH_UyDQ0wwX_6-SPkQ978dlOyW48hTG3aPnt0BqhJyEIXk8X_JhW3chQZm5KqElOOnIZ1FXL8E1" alt="" title="artflow 5"></p>

